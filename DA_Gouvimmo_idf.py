
import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import folium
from folium.plugins import MarkerCluster
from streamlit_folium import folium_static
import os
from joblib import load
from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error

st.set_page_config(
        page_title="√âtude de l'immobilier en France",
        page_icon="üè†",  # Replace this with the path to your icon file
        layout="wide"
    )

st.title("ETUDE DU MARCHE DE L‚ÄôIMMOBILIER EN FRANCE")
st.sidebar.title("üè† Etude Immobilier")
pages=["Le Projet", "Le Jeu de donn√©es", "DataVizualization", "Mod√©lisation","Conclusion & Perspective"]
page=st.sidebar.radio("Aller vers", pages)

col = st.columns((4.5, 2), gap='medium')

with col[0]:
 if page == pages[0] : 
  st.write("### Introduction Du Projet")
  st.write("""
 <div style="text-align: justify;">
  <p>  Ce projet consiste √† analyser le march√© immobilier fran√ßais afin de comprendre ses tendances, anticiper les √©volutions futures et conseiller les parties prenantes. Il inclut la collecte, le nettoyage et l'analyse des donn√©es de 2018 √† 2023, en tenant compte de facteurs tels que les politiques √©conomiques, les taux d'int√©r√™t, la d√©mographie et les avanc√©es technologiques. Sur le plan scientifique, il applique des m√©thodes statistiques et d'apprentissage automatique pour analyser les tendances, pr√©dire les d√©veloppements futurs et fournir des insights pour la prise de d√©cision.

    Dans le pr√©traitement des donn√©es, une normalisation et le traitement des valeurs manquantes et aberrantes sont effectu√©s. Le feature engineering permettra de cr√©er de nouvelles variables pertinentes pour l'analyse, comme le prix au m√®tre carr√© et la cat√©gorisation des DPE en fonction de l'√©volution des prix des biens par r√©gions. Des outils de visualisation tels que Matplotlib, Seaborn et Power BI seront utilis√©s pour pr√©senter l'analyse et repr√©senter les tendances et les relations dans les donn√©es. Des analyses statistiques aideront √† d√©terminer les facteurs influen√ßant le march√© immobilier pour une meilleure prise de d√©cisions.

    </p>
 </div>
    """, unsafe_allow_html=True)
  st.image("immobilier.jpg")
  
 if page == pages[1] : 
    df=pd.read_csv("GouvImmo_full_MLIdF.csv",sep=',',low_memory=False)
    st.write("### Exploration des donn√©es")
    
    st.dataframe(df.head())
    
    st.write("Dimensions du dataframe :")
    
    st.write(df.shape)
    
    if st.checkbox("Afficher les valeurs manquantes") : 
        st.dataframe(df.isna().sum())
        
    if st.checkbox("Afficher les doublons") : 
        st.write(df.duplicated().sum())
    
 if page == pages[2] : 
    @st.cache_data
    def charger_donnees():
      df = pd.read_csv('GouvImmo_full_MLIdF.csv')
      df['nombre_pieces_principales'] = pd.to_numeric(df['nombre_pieces_principales'], errors='coerce')
      df['surface_reelle_bati'] = pd.to_numeric(df['surface_reelle_bati'], errors='coerce')  # Ensure it is numeric
      df['valeur_fonciere'] = pd.to_numeric(df['valeur_fonciere'], errors='coerce')  # Ensure it is numeric
      # Create 'Prix au m¬≤' safely
      df['Prix au m¬≤'] = df.apply(lambda row: row['valeur_fonciere'] / row['surface_reelle_bati'] if row['surface_reelle_bati'] > 0 else np.nan, axis=1)
      return df
    
    df = charger_donnees()

    df_filtered = df[(df['nombre_pieces_principales'] != 0) & (df['nombre_pieces_principales'] <= 15)]

    # Display the charts based on user selection
    # Select Chart Type
    chart_type = st.sidebar.radio("Choisir le type de graphique:", (
        'Distribution de "type_local_x"',
        'Distribution des Pieces Principales',
        'Surface Distribution par Type',
        'Distribution du Prix au m¬≤',
        '√âvolution de la valeur des biens',
        'Carte des Transactions Immobili√®res'
                                
     ))

    if chart_type == 'Distribution de "type_local_x"':
      fig, ax = plt.subplots()
      sns.countplot(x='type_local_x', data=df_filtered, palette='deep', ax=ax)
      st.pyplot(fig)

    elif chart_type == 'Distribution des Pieces Principales':
      fig, ax = plt.subplots()
      sns.countplot(x='nombre_pieces_principales', data=df_filtered, palette='deep', ax=ax)
      st.pyplot(fig)

    elif chart_type == 'Surface Distribution par Type':
      fig = plt.figure()
      sns.boxplot(x='type_local_x', y='surface_reelle_bati', data=df_filtered)
      plt.xticks(rotation=45)
      st.pyplot(fig)

    
    elif chart_type == 'Distribution du Prix au m¬≤':
      df_filtered['Prix au m2'] = df_filtered['valeur_fonciere'] / df_filtered['surface_reelle_bati']
      fig, ax = plt.subplots()
      sns.distplot(df_filtered['Prix au m2'].dropna(), bins=50, kde=False)
      st.pyplot(fig)
      
    elif chart_type == '√âvolution de la valeur des biens':
       # Convert 'date_mutation' to datetime format
       df['date_mutation'] = pd.to_datetime(df['date_mutation'])
       
       # Extract year from 'date_mutation' and create a new column
       df['year'] = df['date_mutation'].dt.year
       
       # Group by year and calculate mean and median price per m¬≤
       price_per_sqm_yearly = df.groupby('year')['Prix au m¬≤(‚Ç¨)'].agg(['mean', 'median']).reset_index()
       
       # Create a line plot using Plotly
       fig2 = px.line(price_per_sqm_yearly, x='year', y=['mean', 'median'],
                     labels={'value':'Price per m¬≤ (‚Ç¨)', 'variable':'Statistic'},
                     title='Trends in Property Values (Price per m¬≤) Over Time in Ile-de-France')
       
       # Update traces to have different markers
       fig2.update_traces(mode='lines+markers')
       
       # Add more details to the layout
       fig2.update_layout(xaxis_title='Year',
                         yaxis_title='Price per m¬≤ (‚Ç¨)',
                         legend_title='Statistic',
                         legend=dict(y=1, x=1),
                         margin=dict(l=20, r=20, t=30, b=20))
       
       # Show the figure
       st.plotly_chart(fig2)
       
    elif chart_type == 'Carte des Transactions Immobili√®res':
        
        dataV = df[df['code_postal'] == 78000]
        
        @st.cache_data
        def create_map(dataV):
            # Cr√©ation de l'objet carte centr√© sur Versailles
            m = folium.Map(location=[48.8047, 2.1204], zoom_start=12)

            # Utilisation de MarkerCluster pour g√©rer de nombreux marqueurs
            marker_cluster = MarkerCluster().add_to(m)

            # Boucle sur les donn√©es pour ajouter des marqueurs
            for idx, row in dataV.iterrows():
                # Cr√©ation du texte pour la popup
                popup_text = f"""
                <strong>Date:</strong> {row['date_mutation']}<br>
                <strong>Adresse:</strong> {row['Adresse']}<br>
                <strong>Prix/m¬≤:</strong> {row['Prix au m¬≤(‚Ç¨)']}‚Ç¨<br>
                <strong>Surface:</strong> {row['surface_reelle_bati']} m¬≤<br>
                <strong>Prix total:</strong> {row['valeur_fonciere']}‚Ç¨
                """
                folium.Marker(
                    location=[row['latitude'], row['longitude']],
                    popup=popup_text,
                    icon=folium.Icon(icon="info-sign")
                ).add_to(marker_cluster)

            return m

        # Cr√©ation de la carte
        map_obj = create_map(dataV)
        
        st.title('Carte des Transactions Immobili√®res')
        st.write('Voici une carte interactive montrant les transactions immobili√®res en se basant sur la date de mutation, l‚Äôadresse, et les d√©tails financiers de chaque transaction.')
        folium_static(map_obj)
        
 if page == pages[3] :
    # Define the base path to the model directory
    BASE_PATH = r'C:\Users\Akshit\DA\idf'  # Ensure this is the correct path where your models are stored
    
    # Possible choices for regions and models
    department = [
        "Essonne", "Val-d'Oise", "Seine-et-Marne", "Hauts-de-Seine",
        "Yvelines", "Val-de-Marne", "Seine-Saint-Denis",
        "Paris"
    ]
    models = ["Decision Tree", "Linear Regression", "Gradient Boosting"]
    
    @st.cache_data
    def load_model(department, model_name):
        """ Load the model based on the region and model name. """
        filename = f"{department.replace(' ', '_')}_{model_name.replace(' ', '_')}_model.joblib"
        model_path = os.path.join(BASE_PATH, filename)
        if not os.path.exists(model_path):
            st.error(f"Model file not found: {model_path}")
            return None
        return load(model_path)
    
    @st.cache_data
    def prepare_data_for_department(department, full_dataset):
        """ Prepare data for the given region. Assumes full_dataset includes 'nom_region' as a column. """
        if 'nom_departement' not in full_dataset or 'Prix au m¬≤(‚Ç¨)' not in full_dataset:
            st.error("Required columns are missing from the dataset.")
            return None, None
        department_data = full_dataset[full_dataset['nom_departement'] == department]
        if department_data.empty:
            st.error("No data found for the selected region.")
            return None, None
        X = department_data.drop('Prix au m¬≤(‚Ç¨)', axis=1, errors='ignore')  # Safely drop target column
        y = department_data['Prix au m¬≤(‚Ç¨)']
        return X, y
    
    try:
        # Load the full dataset
        full_dataset = pd.read_csv('GouvImmo_full_MLIdF.csv', sep=',', low_memory=False)
    except Exception as e:
        st.error(f"Failed to load data: {str(e)}")
        full_dataset = None
    
    st.title('Interface de pr√©vision des prix immobiliers')
    
    if full_dataset is not None:
        # Selection boxes for regions and models
        selected_department = st.selectbox('Select department', department)
        selected_model = st.selectbox('Select Model', models)
    
        if st.button('Predict'):
            model = load_model(selected_department, selected_model)
            if model is not None:
                X_department, y_true_department = prepare_data_for_department(selected_department, full_dataset)
                if X_department is not None and y_true_department is not None:
                    y_pred_department = model.predict(X_department)
                    mae = mean_absolute_error(y_true_department, y_pred_department)
                    mape = mean_absolute_percentage_error(y_true_department, y_pred_department) 
                    st.write(f"Mean Absolute Error (MAE): {mae:.2f}")
                    st.write(f"Mean Absolute Percentage Error (MAPE): {mape:.2f}%")
    else:
        st.write("Please check the data file path and ensure it's correct.")

 if page == pages[4] :
  st.write("""
 <div style="text-align: justify;">
  <p>  Une piste d‚Äôam√©lioration qui pourrait permettre selon nous d‚Äôaugmenter les performances du mod√®le serait justement d‚Äôobtenir les informations compl√©mentaires pouvant impacter le prix d‚Äôun bien. Une partie pourrait provenir des sites d‚Äôagences immobili√®res (se loger, bienici, pap, leboncoin etc), et il faudrait donc effectuer du webscrapping par exemple pour les r√©cup√©rer. Cependant, l√† encore il y aurait plusieurs limites √† cette piste :
       Tous les biens vendus n‚Äôont pas forc√©ment √©t√© mis en vente sur un site d‚Äôagence immobili√®re
       On aurait toujours le probl√®me de la cl√© commune, comme les informations n√©cessaires comme l‚Äôadresse ne sont pas disponibles sur ces sites.
       Il faudrait √™tre capable de retrouver ces donn√©es de fa√ßon r√©troactive,
       C‚Äôest -√† -dire retrouver les annonces des mois apr√®s que les biens sont vendus.
       Une piste d‚Äôam√©lioration qui pourrait √™tre possible mais que nous n‚Äôavons pas int√©gr√© par manque de temps serait de lier des donn√©es externes comme la pr√©sence d‚Äô√©coles, transports, parcs etc. Cela pourrait √™tre des donn√©es suppl√©mentaires pouvant impacter le prix de vente des biens, m√™me si l√† encore la liaison ne serait que relative (peut-√™tre effectuer une liaison via la rue par exemple ?)
    </p>
 </div>
    """, unsafe_allow_html=True)  
  img_col1, img_col2, img_col3 = st.columns(3)

  with img_col1:
        st.image("eric.jpg", use_column_width=False, caption='Eric')

  with img_col2:
        st.image("dev.jpg", use_column_width=False, caption='Marc')

  with img_col3:
        st.image("momo.jpg", use_column_width=False, caption='Mohamed')
   
 with col[1]:
  with st.expander('About', expanded=True):
        st.write('''
            - Data: [Data.Gouv.fr](https://files.data.gouv.fr/geo-dvf/latest/csv/).
            - :orange[**DataScientest**]: Ce projet a √©t√© r√©alis√© dans le cadre de notre formation en data science via l'organisme Datascientest.
            - :green[**Participants**]: √âric LOUGUET Marc DEVADEVAN Mohamed KEITA
                 ''')
            