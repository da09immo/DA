
import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import folium
from folium.plugins import MarkerCluster
from streamlit_folium import folium_static
import os
from joblib import load
from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error

st.set_page_config(
        page_title="√âtude de l'immobilier en France",
        page_icon="üè†",  # Replace this with the path to your icon file
        layout="wide"
    )

st.title("ETUDE DU MARCHE DE L‚ÄôIMMOBILIER EN FRANCE")
st.sidebar.title("üè† Etude Immobilier")
pages=["Le Projet", "Le Jeu de donn√©es", "DataVizualization", "Mod√©lisation","Conclusion & Perspective"]
page=st.sidebar.radio("Aller vers", pages)

col = st.columns((4.5, 2), gap='medium')

with col[0]:
 if page == pages[0] : 
  st.write("### Introduction Du Projet")
  
  st.image("immobilier.jpg", width=500)
  st.write("""
 <div style="text-align: justify;">
  <p>  Ce projet consiste √† analyser le march√© immobilier fran√ßais afin de comprendre ses tendances, anticiper les √©volutions futures et conseiller les parties prenantes. Il inclut la collecte, le nettoyage et l'analyse des donn√©es de 2018 √† 2023, en tenant compte de facteurs tels que les politiques √©conomiques, les taux d'int√©r√™t, la d√©mographie et les avanc√©es technologiques. Sur le plan scientifique, il applique des m√©thodes statistiques et d'apprentissage automatique pour analyser les tendances, pr√©dire les d√©veloppements futurs et fournir des insights pour la prise de d√©cision.

    Dans le pr√©traitement des donn√©es, une normalisation et le traitement des valeurs manquantes et aberrantes sont effectu√©s. Le feature engineering permettra de cr√©er de nouvelles variables pertinentes pour l'analyse, comme le prix au m√®tre carr√© et la cat√©gorisation des DPE en fonction de l'√©volution des prix des biens par r√©gions. Des outils de visualisation tels que Matplotlib, Seaborn et Power BI seront utilis√©s pour pr√©senter l'analyse et repr√©senter les tendances et les relations dans les donn√©es. Des analyses statistiques aideront √† d√©terminer les facteurs influen√ßant le march√© immobilier pour une meilleure prise de d√©cisions.

    </p>
 </div>
    """, unsafe_allow_html=True)
  
  
  
 if page == pages[1] : 
    df=pd.read_csv("GouvImmo_full_MLIdF.csv",sep=',',low_memory=False)
    st.write("### Exploration des donn√©es")
    
    st.dataframe(df.head())
    
    st.write("Dimensions du dataframe :")
    
    st.write(df.shape)
    
    if st.checkbox("Afficher les valeurs manquantes") : 
        st.dataframe(df.isna().sum())
        
    if st.checkbox("Afficher les doublons") : 
        st.write(df.duplicated().sum())
    
 if page == pages[2] : 
    @st.cache_data
    def charger_donnees():
      df = pd.read_csv('GouvImmo_full_MLIdF.csv')
      df['nombre_pieces_principales'] = pd.to_numeric(df['nombre_pieces_principales'], errors='coerce')
      df['surface_reelle_bati'] = pd.to_numeric(df['surface_reelle_bati'], errors='coerce')  # Ensure it is numeric
      df['valeur_fonciere'] = pd.to_numeric(df['valeur_fonciere'], errors='coerce')  # Ensure it is numeric
      # Create 'Prix au m¬≤' safely
      df['Prix au m¬≤'] = df.apply(lambda row: row['valeur_fonciere'] / row['surface_reelle_bati'] if row['surface_reelle_bati'] > 0 else np.nan, axis=1)
      return df
    
    df = charger_donnees()

    df_filtered = df[(df['nombre_pieces_principales'] != 0) & (df['nombre_pieces_principales'] <= 15)]

    # Display the charts based on user selection
    # Select Chart Type
    chart_type = st.sidebar.radio("Choisir le type de graphique:", (
        'Distribution de "type_local_x"',
        'Distribution des Pieces Principales',
        'Surface Distribution par Type',
        'Distribution du Prix au m¬≤',
        '√âvolution de la valeur des biens',
        'Carte des Transactions Immobili√®res'
                                
     ))

    if chart_type == 'Distribution de "type_local_x"':
      fig = px.histogram(df_filtered, x='type_local_x', color='type_local_x')
      st.plotly_chart(fig)

    elif chart_type == 'Distribution des Pieces Principales':
      custom_colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']
      fig = px.histogram(df_filtered, x='nombre_pieces_principales', color_discrete_sequence=custom_colors)
      st.plotly_chart(fig)

    elif chart_type == 'Surface Distribution par Type':
      fig = px.box(df_filtered, x='type_local_x', y='surface_reelle_bati')
      fig.update_layout(xaxis_tickangle=-45)  # Rotate x-axis labels
      st.plotly_chart(fig)

    
    elif chart_type == 'Distribution du Prix au m¬≤':
      df_filtered['Prix au m2'] = df_filtered['valeur_fonciere'] / df_filtered['surface_reelle_bati']
      fig = px.histogram(df_filtered, x='Prix au m2', nbins=50)
      fig.update_layout(xaxis_range=[0, 25000])  # Set x-axis range
      st.plotly_chart(fig)
      
    elif chart_type == '√âvolution de la valeur des biens':
       # Convert 'date_mutation' to datetime format
       df['date_mutation'] = pd.to_datetime(df['date_mutation'])
       
       # Extract year from 'date_mutation' and create a new column
       df['year'] = df['date_mutation'].dt.year
       
       # Group by year and calculate mean and median price per m¬≤
       price_per_sqm_yearly = df.groupby('year')['Prix au m¬≤(‚Ç¨)'].agg(['mean', 'median']).reset_index()
       
       # Create a line plot using Plotly
       fig2 = px.line(price_per_sqm_yearly, x='year', y=['mean', 'median'],
                     labels={'value':'Price per m¬≤ (‚Ç¨)', 'variable':'Statistic'},
                     title='Tendances des valeurs immobili√®res (Prix par m¬≤) au fil du temps en √éle-de-France')
       
       # Update traces to have different markers
       fig2.update_traces(mode='lines+markers')
       
       # Add more details to the layout
       fig2.update_layout(xaxis_title='Year',
                         yaxis_title='Price per m¬≤ (‚Ç¨)',
                         legend_title='Statistic',
                         legend=dict(y=1, x=1),
                         margin=dict(l=20, r=20, t=30, b=20))
       
       # Show the figure
       st.plotly_chart(fig2)
       
    elif chart_type == 'Carte des Transactions Immobili√®res':
        
        data_versailles_filtered = pd.read_csv('data_versailles_sorted.csv')
        
        @st.cache_data
        def create_map(data):
            m = folium.Map(location=[48.8047, 2.1204], zoom_start=12)  # Centre sur Versailles
            marker_cluster = MarkerCluster().add_to(m)
            for idx, row in data.iterrows():
                # Prix pr√©dit arrondi √† l'euro pr√®s
                prix_pred_euro = round(row['Prix pr√©dit au m¬≤(‚Ç¨)'])
                
                # Texte pour le popup
                popup_text = f"""
                <strong>Date:</strong> {row['date_mutation']}<br>
                <strong>Adresse:</strong> {row['Adresse']}<br>
                <strong>Prix/m¬≤ r√©el:</strong> {row['Prix au m¬≤(‚Ç¨)']}‚Ç¨<br>
                <strong>Prix/m¬≤ pr√©dit:</strong> {prix_pred_euro}‚Ç¨<br>
                <strong>Surface r√©elle b√¢tie:</strong> {row['surface_reelle_bati']} m¬≤<br>
                <strong>Valeur fonci√®re:</strong> {row['valeur_fonciere']}‚Ç¨<br>
                """
                folium.Marker(
                    location=[row['latitude'], row['longitude']],
                    popup=popup_text,
                    icon=folium.Icon(icon="info-sign")
                ).add_to(marker_cluster)
            return m
        
        # Widget pour modifier le prix au m¬≤
        st.sidebar.subheader("Param√®tres de la carte")
        min_price_pred = int(data_versailles_filtered['Prix pr√©dit au m¬≤(‚Ç¨)'].min())
        max_price_pred = int(data_versailles_filtered['Prix pr√©dit au m¬≤(‚Ç¨)'].max())
        price_pred_range = st.sidebar.slider(
            "S√©lectionnez la plage de prix pr√©dit au m¬≤ (‚Ç¨)",
            min_price_pred, 
            max_price_pred, 
            (min_price_pred, max_price_pred)
        )

        ## Filtrer les donn√©es en fonction du prix au m¬≤ pr√©dit s√©lectionn√©
        filtered_data = data_versailles_filtered[(data_versailles_filtered['Prix pr√©dit au m¬≤(‚Ç¨)'] >= price_pred_range[0]) & 
                                                 (data_versailles_filtered['Prix pr√©dit au m¬≤(‚Ç¨)'] <= price_pred_range[1])]

        # Affichage de la carte avec les donn√©es filtr√©es
        st.subheader("Carte des Transactions Immobili√®res √† Versailles")
        map_obj = create_map(filtered_data)
        folium_static(map_obj)
        
 if page == pages[3] :
    # Define the base path to the model directory
    BASE_PATH = './idf'  # Ensure this is the correct path where your models are stored
    
    # Possible choices for regions and models
    department = [
        "Essonne", "Val-d'Oise", "Seine-et-Marne", "Hauts-de-Seine",
        "Yvelines", "Val-de-Marne", "Seine-Saint-Denis",
        "Paris"
    ]
    models = ["Decision Tree", "Linear Regression", "Gradient Boosting"]
    
    @st.cache_data
    def load_model(department, model_name):
        """ Load the model based on the region and model name. """
        filename = f"{department.replace(' ', '_')}_{model_name.replace(' ', '_')}_model.joblib"
        model_path = os.path.join(BASE_PATH, filename)
        if not os.path.exists(model_path):
            st.error(f"Model file not found: {model_path}")
            return None
        return load(model_path)
    
    @st.cache_data
    def prepare_data_for_department(department, full_dataset):
        """ Prepare data for the given region. Assumes full_dataset includes 'nom_region' as a column. """
        if 'nom_departement' not in full_dataset or 'Prix au m¬≤(‚Ç¨)' not in full_dataset:
            st.error("Required columns are missing from the dataset.")
            return None, None
        department_data = full_dataset[full_dataset['nom_departement'] == department]
        if department_data.empty:
            st.error("No data found for the selected region.")
            return None, None
        X = department_data.drop('Prix au m¬≤(‚Ç¨)', axis=1, errors='ignore')  # Safely drop target column
        y = department_data['Prix au m¬≤(‚Ç¨)']
        return X, y
    
    try:
        # Load the full dataset
        full_dataset = pd.read_csv('GouvImmo_full_MLIdF.csv', sep=',', low_memory=False)
    except Exception as e:
        st.error(f"Failed to load data: {str(e)}")
        full_dataset = None
    
    st.title('Interface de pr√©vision des prix immobiliers')
    
    if full_dataset is not None:
        # Selection boxes for regions and models
        selected_department = st.selectbox('Select department', department)
        selected_model = st.selectbox('Select Model', models)
    
        if st.button('Predict'):
            model = load_model(selected_department, selected_model)
            if model is not None:
                X_department, y_true_department = prepare_data_for_department(selected_department, full_dataset)
                if X_department is not None and y_true_department is not None:
                    y_pred_department = model.predict(X_department)
                    mae = mean_absolute_error(y_true_department, y_pred_department)
                    mape = mean_absolute_percentage_error(y_true_department, y_pred_department) 
                    st.write(f"Mean Absolute Error (MAE): {mae:.2f}")
                    st.write(f"Mean Absolute Percentage Error (MAPE): {mape:.2f}%")
    else:
        st.write("Please check the data file path and ensure it's correct.")

 if page == pages[4] :
  st.write("""
 <div style="text-align: justify;">
  <p>  Une piste d‚Äôam√©lioration qui pourrait permettre selon nous d‚Äôaugmenter les performances du mod√®le serait justement d‚Äôobtenir les informations compl√©mentaires pouvant impacter le prix d‚Äôun bien. Une partie pourrait provenir des sites d‚Äôagences immobili√®res (se loger, bienici, pap, leboncoin etc), et il faudrait donc effectuer du webscrapping par exemple pour les r√©cup√©rer. Cependant, l√† encore il y aurait plusieurs limites √† cette piste :
       Tous les biens vendus n‚Äôont pas forc√©ment √©t√© mis en vente sur un site d‚Äôagence immobili√®re
       On aurait toujours le probl√®me de la cl√© commune, comme les informations n√©cessaires comme l‚Äôadresse ne sont pas disponibles sur ces sites.
       Il faudrait √™tre capable de retrouver ces donn√©es de fa√ßon r√©troactive,
       C‚Äôest -√† -dire retrouver les annonces des mois apr√®s que les biens sont vendus.
       Une piste d‚Äôam√©lioration qui pourrait √™tre possible mais que nous n‚Äôavons pas int√©gr√© par manque de temps serait de lier des donn√©es externes comme la pr√©sence d‚Äô√©coles, transports, parcs etc. Cela pourrait √™tre des donn√©es suppl√©mentaires pouvant impacter le prix de vente des biens, m√™me si l√† encore la liaison ne serait que relative (peut-√™tre effectuer une liaison via la rue par exemple ?)
    </p>
 </div>
    """, unsafe_allow_html=True)  
  img_col1, img_col2, img_col3 = st.columns(3)

  with img_col1:
        st.image("eric.jpg", use_column_width=False)
        st.write("[Eric LOUGUET](https://www.linkedin.com/in/eric-louguet)")

  with img_col2:
        st.image("dev.jpg", use_column_width=False)
        st.write("[Marc DEVADEVAN](https://www.linkedin.com/in/marcdevadevan/)")

  with img_col3:
        st.image("Mohamed.jpg", use_column_width=False)
        st.write("[Mohamed KEITA](https://www.linkedin.com/in/mokeita1/)")
   
 with col[1]:
  with st.expander('About', expanded=True):
        st.write('''
            - Data: [Data.Gouv.fr](https://files.data.gouv.fr/geo-dvf/latest/csv/).
            - :orange[**DataScientest**]: Ce projet a √©t√© r√©alis√© dans le cadre de notre formation en data science via l'organisme Datascientest.
            - :green[**Participants**]: √âric LOUGUET Marc DEVADEVAN Mohamed KEITA
                 ''')
            
